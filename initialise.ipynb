{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functions as fn\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: only need to run this once - creates a file containing all the processed information ready for annotation\n",
    "file_path = \"./data/mimic-iv-note-deidentified-free-text-clinical-notes-2.2/note/\"\n",
    "df = pd.read_csv(file_path+\"discharge.csv\")\n",
    "all_cards = []\n",
    "for i in range(len(df)):\n",
    "    processor = fn.AnnotationProcess(df[\"text\"][i])\n",
    "    cards = processor.extract_HOPC()\n",
    "    if cards is not None:  # only append cards if it's not None\n",
    "        all_cards.append(cards)\n",
    "    # if all_cards has 10000 entries, save and clear the list\n",
    "    if len(all_cards) == 10000:\n",
    "        file_num = i // 10000 + 1  # compute the file number\n",
    "        with open(f'./data/HOPC/HOPC_{file_num}.jsonl', \"w\") as f:\n",
    "            for item in all_cards:\n",
    "                json.dump(item, f)  # write each item on its own line\n",
    "                f.write('\\n')  # add a newline after each item\n",
    "        all_cards = []  # reset the all_cards list for the next batch\n",
    "\n",
    "# save remaining cards, if any\n",
    "if all_cards:\n",
    "    file_num = len(df) // 10000 + 1  # compute the file number\n",
    "    with open(f'./data/HOPC/HOPC_{file_num}.jsonl', \"w\") as f:\n",
    "        for item in all_cards:\n",
    "            json.dump(item, f)  # write each item on its own line\n",
    "            f.write('\\n')  # add a newline after each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 label(s): symptoms, negative_symptoms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/andrewxu/miniforge3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/andrewxu/miniforge3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/prodigy/__main__.py\", line 62, in <module>\n",
      "    controller = recipe(*args, use_plac=True)\n",
      "  File \"cython_src/prodigy/core.pyx\", line 374, in prodigy.core.recipe.recipe_decorator.recipe_proxy\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/plac_core.py\", line 367, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/plac_core.py\", line 232, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/prodigy/recipes/ner.py\", line 157, in manual\n",
      "    nlp = load_model(spacy_model)\n",
      "  File \"cython_src/prodigy/util.pyx\", line 634, in prodigy.util.load_model\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/spacy/__init__.py\", line 54, in load\n",
      "    return util.load_model(\n",
      "  File \"/Users/andrewxu/Documents/2023/MD2/Austin_Research/Code/irAE-detection/env/lib/python3.9/site-packages/spacy/util.py\", line 449, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'BERT_model'. It doesn't seem to be a Python package or a valid path to a data directory.\n"
     ]
    }
   ],
   "source": [
    "dir = \"./data\"\n",
    "label_list = [\n",
    "   \"symptoms\",\n",
    "   \"negative_symptoms\"\n",
    "]\n",
    "database = \"irAE\"\n",
    "model = \"en_core_web_sm\"\n",
    "annotate = fn.ProdigyFunctions(dir, label_list, database, model)\n",
    "\n",
    "annotate.database_connect()\n",
    "\n",
    "annotate.prodigy_connect(\"ner.manual\", \"./data/HOPC/HOPC_1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "========================= Generating Prodigy config =========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-03 20:49:55,749] [INFO] Set up nlp object from config\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 68 | Evaluation: 16 (20% split)\n",
      "Training: 68 | Evaluation: 16\n",
      "Labels: ner (5)\n",
      "[2023-07-03 20:49:55,791] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2023-07-03 20:49:55,793] [INFO] Created vocabulary\n",
      "[2023-07-03 20:49:55,793] [INFO] Finished initializing nlp object\n",
      "[2023-07-03 20:49:56,733] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 68 | Evaluation: 16 (20% split)\n",
      "Training: 68 | Evaluation: 16\n",
      "Labels: ner (5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    218.91    0.00    0.00    0.00    0.00\n",
      "  3     200       2944.82   7531.03   18.18   28.00   13.46    0.18\n",
      "  6     400        868.74   1695.97   41.21   43.16   39.42    0.41\n",
      "  9     600       4138.34   1045.00   47.42   51.11   44.23    0.47\n",
      " 12     800       4053.03   1051.52   47.25   55.13   41.35    0.47\n",
      " 16    1000        891.84    673.91   50.78   55.06   47.12    0.51\n",
      " 19    1200       2365.28    480.07   42.42   44.68   40.38    0.42\n",
      " 22    1400       1852.96    458.16   41.76   48.72   36.54    0.42\n",
      " 25    1600       5384.54    421.15   44.57   51.25   39.42    0.45\n",
      " 29    1800       4456.28    370.64   49.26   50.51   48.08    0.49\n",
      " 32    2000        577.88    272.30   47.06   53.01   42.31    0.47\n",
      " 36    2200        312.87    186.10   53.00   55.21   50.96    0.53\n",
      " 39    2400        419.50    183.02   49.29   48.60   50.00    0.49\n",
      " 43    2600      26277.67    318.02   46.63   50.56   43.27    0.47\n",
      " 46    2800        539.82    192.12   47.73   58.33   40.38    0.48\n",
      " 50    3000       9717.88    212.08   50.00   54.55   46.15    0.50\n",
      " 54    3200        435.50    156.20   50.72   50.48   50.96    0.51\n",
      " 58    3400        822.79    178.27   44.10   47.25   41.35    0.44\n",
      " 62    3600        566.47    186.61   45.16   51.22   40.38    0.45\n",
      " 68    3800        485.64    176.86   44.98   44.76   45.19    0.45\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "models/model-last\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "output_dir = \"./models/\"\n",
    "ner_data = \"irAE\"\n",
    "\n",
    "os.system(f\"prodigy train ./models/ --ner irAE --config config.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 label(s): symptoms, negative_symptoms\n",
      "\u001b[38;5;3m⚠ The model you're using isn't setting sentence boundaries (e.g. via\n",
      "the parser or sentencizer). This means that incoming examples won't be split\n",
      "into sentences.\u001b[0m\n",
      "\n",
      "✨  Starting the web server at http://localhost:8080 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.system(\"prodigy ner.correct irAE ./models/model-last/ ./data/HOPC/HOPC_3.jsonl --label symptoms,negative_symptoms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
